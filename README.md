# RAG-Powered_LogQA
## Overview

This project develops a Question-Answering (Q&A) system that leverages web traffic logs to answer user queries. The system is built on the **Retrieval-Augmented Generation (RAG)** model, which retrieves relevant information from traffic logs and generates accurate responses to user questions.

You can use this project with either synthetic log data, generated by the script provided, or your own web traffic logs.

## Key Features

- **Synthetic Log Generation**: A script is provided to generate realistic, diverse web traffic logs using the `Faker` library. The generated logs simulate various user interactions, including geographic locations, IP addresses, HTTP methods, URLs, browser user agents, referrers, and session information.
- **Retrieval-Augmented Generation (RAG)**: The Q&A system retrieves relevant information from the log data and generates suitable answers to user queries based on those logs.

## Prerequisites

Before running the code, ensure you have the required dependencies installed. You can install them using the `requirements.txt` provided in the repo:

```bash

pip install -r requirements.txt

```

## Usage

### 1. Generating Synthetic Web Traffic Logs

If you do not have access to your own web traffic logs, you can generate synthetic logs using the provided script. This script simulates real-world user interactions on a website and generates logs in a format similar to those used by Apache or Nginx servers.

To generate synthetic log data:

```bash

python synthetic_data.py

```

By default, this will generate 10,000 log entries and save them to `diverse_synthetic_web_traffic.log`. You can modify the number of entries as needed by passing a different value to the `num_entries` parameter.

### 2. Using Your Own Log Data

If you already have your own web traffic logs, you can skip the synthetic log generation and proceed directly to using the logs with the Q&A system. Ensure that your logs are formatted correctly (e.g., Apache/Nginx log format) so that they can be properly parsed and analyzed by the system.

### Example Log Entry

Hereâ€™s an example of a synthetic log entry generated by the script:

```arduino

192.168.56.12 - 5a1fcf10-e89b-11eb-9a03-0242ac130003 [19/Aug/2023:10:15:32 +0000] "GET /product HTTP/1.1" 200 12345 "https://google.com" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36" Cookie="5f4dcc3b5aa765d61d8327deb882cf99"

```

This log entry includes details like:

- IP Address
- Session ID
- Timestamp
- HTTP Method and URL
- Status Code
- Referrer URL
- User Agent
- Cookie Information

## File Structure

- `synthetic_data.py`: Script for generating synthetic log data.
- `requirements.txt`: List of dependencies required for the project.
- `diverse_synthetic_web_traffic.log`: Example of synthetic log data (if generated).

## Log Data Cleaning and Preparation (cleaning_editing.py)

The `cleaning_editing.py` script is designed to clean and prepare the log data for further processing and analysis. After generating or importing the log data, this script ensures that the data is of high quality, free from errors, inconsistencies, and missing values. Here's an overview of what this script does:

### Key Features:

1. **Parsing Log Data**:
    - The script reads log entries from the file and parses each entry into individual components (IP, session ID, timestamp, method, URL, status code, etc.).
2. **Data Cleaning**:
    - Handles missing or invalid data, including IP addresses, timestamps, and HTTP methods.
    - Validates HTTP status codes, request sizes, and ensures the log data is consistent and correct.
3. **Data Enrichment**:
    - Extracts additional insights from the data, such as the hour and day from the timestamp.
    - Converts request sizes to megabytes for easier analysis.
4. **Outlier Detection**:
    - Identifies and removes outliers based on the request size (in MB) using Z-scores.
5. **Standardization**:
    - Ensures consistency by converting HTTP methods to uppercase.
6. **Reporting**:
    - Reports missing values and inconsistencies in the data after cleaning.

### How to Use

To use the `cleaning_editing.py` script, follow these steps:

1. **Prepare the Log Data**:
    - Ensure you have generated or imported log data in the proper format. You can use either the synthetic logs from `synthetic_data.py` or your own logs.
2. **Run the Script**:
    - The script reads the log data from a file, cleans and validates it, and then saves the cleaned data in both CSV and Excel formats.
    
    ```bash
    bashKodu kopyala
    python cleaning_editing.py
    
    ```
    
3. **Check the Cleaned Data**:
    - After the script runs, you will find two new files in your project directory:
        - `high_quality_log.csv`
        - `high_quality_log.xlsx`
    - These files contain the cleaned and validated log data, ready for further analysis or use in the Q&A system.

### Example of the Cleaning Process:

- **IP Validation**: Ensures all IP addresses are valid and correctly formatted.
- **Timestamp Validation**: Converts timestamps into proper datetime format, removing invalid entries.
- **HTTP Method Validation**: Filters out invalid or non-standard HTTP methods, such as GET, POST, PUT, DELETE, etc.
- **Outlier Removal**: Request sizes are evaluated, and any outliers (based on Z-scores) are removed to ensure data quality.

### Additional Features:

- **Extracting Hour and Day**: The script enriches the data by extracting the hour of the day and the day of the week from each log entry, allowing for temporal analysis.
- **Missing Value Handling**: Missing values in critical fields (like IP address, timestamp, and URL) are removed, while missing values in non-critical fields (such as referrer and user agent) are replaced with placeholders.

### File Structure

- `cleaning_editing.py`: Script for parsing, cleaning, and validating the log data.
- `high_quality_log.csv`: Cleaned log data in CSV format.
- `high_quality_log.xlsx`: Cleaned log data in Excel format.

